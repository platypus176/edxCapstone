---
title: "MovieLens Project Report"
author: "Dale Chen-Song"
date: "08/06/2020"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
tinytex::install_tinytex()
library(knitr)
library(tinytex)
knitr::opts_chunk$set(echo = TRUE)
```

```{r , include=FALSE, echo = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")

test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

# create test set using 20% of edx data and train set with the rest
set.seed(2, sample.kind = "Rounding")
test_index<-createDataPartition(edx$rating, times = 1, p = 0.2, list = FALSE)
test_set<- edx[test_index,]
train_set<-edx[-test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

## 1. Introduction
Recommendation systems are an important part of many businesses nowadays. As companies collect user informations and ratings, it becomes imperative to the companies to understand and predict the users to make specific recommendations to the users based, so the companies can better sell their products. Therefore, recommendation systems are based on previous user's ratings and uses machine learning to achieve this. The goal of machine learning is to process the data using algorithms to predict.

One big example of this, comes from Netflix. Netflix set out an open challenge, called the Netflix Prize, for the best algorithm to predict user's movie ratings from a large dataset of previous ratings. The winner of the challenge would receive $1M if they could achieve an Root Mean Square Error (RMSE) of 0.8572.

In our report, we create our own recommendation system, similar to the Netflix Prize, and try to achieve the challenge's RMSE target. We divide the dataset so we have a validation set to test our final recommendation system and an edx dataset that we will use to make our recommendation system. We then further divide the edx dataset, so we can use train a set to the test set. We train by looking at bias with movies, then movies with user, and then we finally use regularize our movies and user effect. 

## 2. Summary
While we aren't using the Netflix dataset, the dataset that we use for this project is obtained from [MovieLens dataset](https://grouplens.org/datasets/movielens/10m/). While the Netflix challenge uses 100 millions ratings, we use the 10 million version of the MovieLens dataset to make the computation a little easier.

The dataset contains 10 million ratings with 100,000 tag applications, 72,000 users, and 10,000 movie. We split the dataset so it is 90%-10%; the 90% is the edx dataset, which we'll use to analyze and the 10% is used for validation at the end. 
Both datasets contains 6 categories: the user ID, the movie ID, rating by the user, timestamp when the rating was done, title of the movie, and the genres of the movie.

```{r head, echo=TRUE}
head(edx)
dim(edx)
n_distinct(edx$userId)
n_distinct(edx$movieId)
dim(validation)
```

The edx dataset contains around 9,000,000 ratings, with almost 70,000 different users, and more than 10,000 different movies, while the validation dataset has 999,999 ratings.

Here, we show the top most rated movies. The most rated movie is *Pulp Fiction* (1994) with over 31,000 ratings.

```{r most_rated, echo=TRUE}
edx %>% group_by(title) %>%
  summarize(no_ratings = n()) %>%
  arrange(desc(no_ratings))
```

The most popular movie (with more than 1000 ratings) with an average rating of 4.455 is *The Shawshank Redemption* (1994), with over 28,000 ratings.

```{r most_popular, echo = TRUE}
popular<-edx %>% group_by(movieId, title) %>%
  filter(n() >= 1000) %>%
  summarise(count = n(), average = mean(rating)) %>%
  arrange (desc(average))

head(popular)
```

There are over a hundred films with only one rating that may skew the results.

```{r single_rating, echo = TRUE}
edx %>% group_by(title) %>%
  summarize(no_ratings = n()) %>%
  filter(no_ratings == 1) %>%
  count() %>% 
  pull()
```

Due to the large size of the dataset, we can't use machine learning algorithms from caret as it would use up too many resources. Thus, the method that we use is user and movie effects with regularization.

## 3. Methods/Analysis
From the edx dataset, we further divide the dataset into 80%-20%, the train and test sets respectively. 

The easiest model we can make is by just taking the average and using that to predict against the test set. We find that the average of the training set is 3.512.
Using this mean, we can calculate an RMSE value of 1.062.

```{r mean, echo = TRUE}
mu<-mean(train_set$rating)
RMSE(test_set$rating, mu)
```

As this is an unoptimal RMSE value, we can improve our model by adding an independent error term, the movie bias.
With the addition of movie effect, our model's new RMSE value is 0.9441

```{r movie, echo = TRUE}
# Add movie bias term
b_i<-train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating-mu))

# Predict rating with mean and movie bias term
predicted_ratings<-test_set %>% 
  left_join(b_i, by= 'movieId') %>% 
  mutate(pred=mu+b_i)%>% 
  pull(pred)

# RMSE for movie effect
RMSE(predicted_ratings, test_set$rating)
```

We can further improve on our model by adding an additional bias, users. 
With this addition, the RMSE value is now 0.8669.

```{r movie_user, echo = TRUE}
# Add user bias term
b_u<- train_set %>% 
  left_join(b_i, by='movieId') %>% 
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# Predict rating with mean, movie and user bias term
predicted_ratings<- test_set %>% 
  left_join(b_i, by= 'movieId') %>% 
  left_join(b_u, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>% 
  pull(pred)

# RMSE for movie and user effect
RMSE(predicted_ratings, test_set$rating)
```

As there are many estimates with small sample sizes as noted in Summary, we use regularization to reduce the effects of large errors in our predictions. We apply regularization on both our biases.

```{r regularization, echo = TRUE}
# Figuring out best lambda
lambdas<- seq(0,8,0.25)

# Regularization function
regularization<-function(x){
  # Overall average rating of train dataset
  mu<-mean(train_set$rating)
  
  # Regularized movie bias term
  b_i<- train_set %>% 
    group_by(movieId)%>% 
    summarize(b_i = sum(rating-mu)/(n()+x))
  
  # Regularized user bias term
  b_u<- train_set %>% 
    left_join(b_i, by= 'movieId') %>% 
    group_by(userId) %>% 
    summarize(b_u = sum(rating - b_i-mu)/(n()+x))
  
  # Predict rating with mean, movie and user bias term
  predicted_ratings<- test_set %>% 
    left_join(b_i, by= 'movieId') %>% 
    left_join(b_u, by= 'userId')%>%
    mutate(pred= mu + b_i + b_u) %>% 
    .$pred
  # RMSE for regularized movie and user effect
  return(RMSE(predicted_ratings, test_set$rating))}

# Obtain RMSE of lambda using regularization function
rmse<- sapply(lambdas, regularization)

# Plot RMSE vs. lambdas 
plot(lambdas, rmse)
```

We see that the best lambda for minimal RMSE is 5.

```{r lambda, echo = TRUE}
# Figuring out the minimized lambda
lambda<-lambdas[which.min(rmse)]
lambda
```

From there, we can see that the RMSE for the minimzed lambda and regularization of movie and user effect is 0.8662.

```{r regularized_rmse, echo = TRUE}
# Output RMSE of regularized model
regularization(lambda)
```

## 4. Results
As we have achieved our final model, we use our model on our validation set to obtain our final RMSE. 

```{r validation, echo= TRUE}
# Figuring out best lambda
lambdas<- seq(0,8,0.25)

# Regularization function
regularization<-function(x){
  # Overall average rating of edx dataset
  mu<- mean(edx$rating)
  
  # Regularized movie bias term
  b_i<- edx %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n()+x))
  
  # Regularized user bias term
  b_u<- edx %>% 
    left_join(b_i, by="movieId") %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+x))
  
  # Predictions on validation set using mean and regularized movie and user bias term
  predicted_ratings<- validation %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by="userId") %>%
    mutate(pred= mu +b_i+b_u) %>% 
    pull(pred)
  
  # Final model output RMSE
  return(RMSE(predicted_ratings, validation$rating))}

# Obtain RMSE of lambda using regularization function
rmse<- sapply(lambdas, regularization)

#plot
qplot(lambdas,rmse)
```

From then, we find the best lambda to achieve the minimal RMSE is 5.25. 

```{r lambda_validation, echo = TRUE}
# Figuring out the minimized lambda
lambda<-lambdas[which.min(rmse)]
lambda
```

The RMSE result that we obtain after using the validation set is 0.8648.

```{r validation_rmse, echo = TRUE}
# Output RMSE of regularized model
regularization(lambda)
```

## 5. Conclusion
While we achieved a reasonable RMSE value of 0.8648, it is not as optimized as it could be. We only looked at movie and user effect, however there are more effects that we could have used such as genre and timestamp.

The winner of the Netflix Prize, BellKor's Pragmatic Chaos, obtained a Test RMSE of 0.8567, showing that there is room for more improvement.  BellKor's achieved this level of RMSE by combining multiple approaches. One example is to use timestamp to figure out users' rating, as users rated differently between Fridays vs. Mondays. However, there are limitations that occur while doing this; it would be too much engineering effort for the results that even Netflix doesn't implement this winning algorthim for their recommendation systems.